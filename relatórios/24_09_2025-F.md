### Versões do DeepSeek:
O DeepSeek possui várias versões, dentre elas, a ideal para o projeto seria uma focada em linguagem natural com baixo consumo de recursos (devido as restrições do Raspberry Pi).
1. DeepSeek LLM (7B)
- Especializado em linguagem natural
- Generalista para conversação e texto
- Versões menores (7B) são mais viáveis para Raspberry Pi
- Otimizado para tarefas de texto puro

7B se refere ao número de parâmetros gerados a partir do treinamento, nesse caso, 7 bilhões.
Parâmetros: são como "neurônios" da IA - conexões que o modelo aprendeu durante o treinamento.
Analogia: quanto mais parâmetros, geralmente mais inteligente o modelo, mas também mais pesado.

Tamanho original (sem quantização): ≈ 14-16 GB em precisão total (16-bit ou 32-bit), inviável para o Pi. Além disso, o ideal seria armazenar o modelo em um SSD externo e não em um cartão microSD.
Para contornar esse problema podemos usar a Quantização: Consiste em reduzir a precisão dos parâmetros, de 16 bits para 3-4 bits. Isso reduz o tamanho do modelo de 16GB para 3-4GB. 

Também existe uma outra alternativa: retreinar o DeepSeek:

Fine-tuning (Mais Viável)
  * Ajustar os parâmetros existentes para uma tarefa específica.
  * Como: dar exemplos do que você quer que o modelo aprenda.
  * Requer menos recursos que treinar do zero.

 Treinar do Zero (Muito Difícil)
  * Criar uma nova base de parâmetros desde o início.
  * Requer: dados massivos, GPU poderosa, semanas/meses de treino. 

Dentre essas opções, a única que reduz o impacto no desempenho final é o treinamento do zero. O fine-tunning apenas direciona e dá uma "personalidade" para o DeepSeek; é como ensinar ele a reagir a determinados tipos de entrada com base em exemplos. 
